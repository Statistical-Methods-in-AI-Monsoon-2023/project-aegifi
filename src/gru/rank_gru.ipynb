{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU, LayerNormalization, RNN, GRUCell, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, classification_report, hamming_loss\n",
    "import tensorflow_ranking as tfr\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 60000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_gru():\n",
    "    X = np.load('../../vectorised_data/X_gru.npy')\n",
    "    y = np.load('../../vectorised_data/y.npy')\n",
    "    \n",
    "    # split data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print('Loaded data')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_data(gru=False,w2v=False):\n",
    "    if gru:\n",
    "        return load_gru()\n",
    "    \n",
    "    # X = scipy.sparse.load_npz('vectorised_data/X.npz')\n",
    "    # y = np.load('vectorised_data/y.npy')\n",
    "    \n",
    "    # # split data into train and test\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # print('Loaded data')\n",
    "\n",
    "    # return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankGRU:\n",
    "    def __init__(self, load_models=False):\n",
    "        self.train_time = 0\n",
    "        self.predict_time = 0\n",
    "        self.preds = None\n",
    "        self.params = {\n",
    "            'units': 128,\n",
    "            'dropout': 0.2,\n",
    "            'layers': 2,\n",
    "            'batch_size': 64,\n",
    "            'epochs': 5,\n",
    "            'lr': 0.001,\n",
    "        }\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.batch_size = self.params['batch_size']\n",
    "        self.train_time = 0\n",
    "        self.predict_time = 0\n",
    "        self.create_model(load_models)\n",
    "    \n",
    "    def create_model(self, load_models=False):\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "                logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "            except RuntimeError as e:\n",
    "                # Visible devices must be set before GPUs have been initialized\n",
    "                print(e)\n",
    "        \n",
    "        if load_models:\n",
    "            self.model = tf.keras.models.load_model('./pretrained/binary_gru.keras')\n",
    "            print(self.model.summary())\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "            model.add(SpatialDropout1D(self.params['dropout']))\n",
    "            for i in range(self.params['layers']):\n",
    "                model.add(GRU(self.params['units'], return_sequences=i != self.params['layers']-1, recurrent_dropout=self.params['dropout']))\n",
    "                model.add(LayerNormalization())\n",
    "            model.add(Dense(20, activation='sigmoid'))\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=self.params['lr'])\n",
    "            model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "            print(model.summary())\n",
    "            self.model = model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        st = time.time()\n",
    "        print(\"Fitting model...\")\n",
    "        \n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "        \n",
    "        self.train_time = time.time() - st\n",
    "        print(\"Done fitting model\")\n",
    "        print(f\"Train time: {self.train_time}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        st = time.time()\n",
    "        print(\"Predicting...\")\n",
    "        \n",
    "        self.preds = self.model.predict(X)\n",
    "        # self.preds = np.round(self.preds)\n",
    "        \n",
    "        self.predict_time = time.time() - st\n",
    "        print(f\"Predict time: {self.predict_time}\")\n",
    "        return self.preds\n",
    "    \n",
    "    def write_metrics(self, y_test):\n",
    "        file_name = f'binary_gru_{datetime.now().strftime(\"%Y%m%d%H%M\")}.txt'\n",
    "\n",
    "        file_path = f'./src/gru/metrics/{file_name}'\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(f'Predict time: {self.predict_time}\\n')\n",
    "            f.write(f'Accuracy: {accuracy_score(y_test, self.preds)}\\n')\n",
    "            f.write(f'Hamming Score: {1 - hamming_loss(y_test, self.preds)}\\n')\n",
    "            f.write(f'Jaccard Score: {jaccard_score(y_test, self.preds, average=\"micro\")}\\n')\n",
    "            # f.write(f'Hit Rate: {hit_rate(y_test, self.preds)}\\n')\n",
    "            f.write('Classification Report:\\n')\n",
    "            f.write(f'{classification_report(y_test, self.preds, zero_division=True)}\\n')\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save(f'./src/gru/pretrained/binary_gru.keras')\n",
    "\n",
    "class RankGRURunner:\n",
    "    def __init__(self, load_models=False):\n",
    "        self.load_models = load_models\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = load_data(gru=True)\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.model = RankGRU(load_models=self.load_models)\n",
    "    \n",
    "    def run_training(self):\n",
    "        self.load_data()\n",
    "        self.init_model()\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.model.save_model()\n",
    "    \n",
    "    def run_inference(self):\n",
    "        self.load_data()\n",
    "        self.init_model()\n",
    "        \n",
    "        self.model.predict(self.X_test)\n",
    "        self.model.write_metrics(self.y_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n",
      "WARNING:tensorflow:Layer gru_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 250, 100)          6000000   \n",
      "                                                                 \n",
      " spatial_dropout1d_15 (Spat  (None, 250, 100)          0         \n",
      " ialDropout1D)                                                   \n",
      "                                                                 \n",
      " gru_30 (GRU)                (None, 250, 128)          88320     \n",
      "                                                                 \n",
      " layer_normalization_30 (La  (None, 250, 128)          256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " gru_31 (GRU)                (None, 128)               99072     \n",
      "                                                                 \n",
      " layer_normalization_31 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6190484 (23.61 MB)\n",
      "Trainable params: 6190484 (23.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "gru = RankGRU(load_models=False)\n",
    "X_train, X_test, y_train, y_test = load_data(gru=True)\n",
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 22:41:25.724132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-11-18 22:41:26.264682: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55746fb12f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-18 22:41:26.264746: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-11-18 22:41:26.270625: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-18 22:41:26.387809: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 28s 1s/step - loss: 0.3779 - categorical_accuracy: 0.2167 - val_loss: 0.2447 - val_categorical_accuracy: 0.3300\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 22s 1s/step - loss: 0.2568 - categorical_accuracy: 0.3233 - val_loss: 0.2390 - val_categorical_accuracy: 0.3300\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.2433 - categorical_accuracy: 0.3322 - val_loss: 0.2355 - val_categorical_accuracy: 0.3300\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.2080 - categorical_accuracy: 0.4367 - val_loss: 0.2451 - val_categorical_accuracy: 0.3500\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.1567 - categorical_accuracy: 0.6311 - val_loss: 0.2539 - val_categorical_accuracy: 0.3400\n",
      "Done fitting model\n",
      "Train time: 112.97887563705444\n"
     ]
    }
   ],
   "source": [
    "gru.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "32/32 [==============================] - 6s 160ms/step\n",
      "Predict time: 5.801921367645264\n"
     ]
    }
   ],
   "source": [
    "preds = gru.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds(y_prob, y):\n",
    "    thresholds = []\n",
    "    for prob, true_labels in zip(y_prob, y):\n",
    "        errors = np.sum(((prob > prob[:, None]) & (true_labels == 0) | (prob <= prob[:, None]) & (true_labels == 1)) , axis=1)\n",
    "        thresholds.append(prob[np.argmin(errors)])\n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = get_thresholds(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(verbosity=2, tree_method=\"hist\", n_jobs=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(preds, thresholds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "7/7 [==============================] - 1s 160ms/step\n",
      "Predict time: 1.2277288436889648\n"
     ]
    }
   ],
   "source": [
    "preds = gru.predict(X_test)\n",
    "thresholds = get_thresholds(preds, y_test)\n",
    "y_thresh = reg.predict(preds)\n",
    "y_pred = (preds >= y_thresh[:, None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14285714 0.         0.14285714 0.         0.         0.14285714\n",
      "  0.         0.14285714 0.         0.         0.         0.\n",
      "  0.14285714 0.         0.         0.14285714 0.         0.\n",
      "  0.         0.14285714]\n",
      " [0.         0.125      0.         0.         0.125      0.\n",
      "  0.125      0.         0.         0.125      0.         0.125\n",
      "  0.         0.125      0.         0.         0.125      0.\n",
      "  0.125      0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_labels(y):\n",
    "    \"\"\"\n",
    "    Transform multi-label output array.\n",
    "\n",
    "    Parameters:\n",
    "    y (numpy.ndarray): Input array with shape (num_samples, num_classes).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Transformed array.\n",
    "    \"\"\"\n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    # Initialize the transformed array\n",
    "    transformed_y = np.zeros_like(y, dtype=float)\n",
    "    \n",
    "    for row_idx in range(y.shape[0]):\n",
    "        # Find indices of correct genres in the current row\n",
    "        correct_indices = np.where(y[row_idx] == 1)[0]\n",
    "        num_correct_genres = len(correct_indices)\n",
    "        \n",
    "        # Set target probability for each correct genre\n",
    "        if num_correct_genres > 0:\n",
    "            transformed_y[row_idx, correct_indices] = 1.0 / num_correct_genres\n",
    "    \n",
    "    return transformed_y\n",
    "\n",
    "# Example usage:\n",
    "# Assuming y is your input array\n",
    "y = np.array([[1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1],\n",
    "              [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0]])\n",
    "\n",
    "transformed_y = transform_labels(y)\n",
    "print(transformed_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
