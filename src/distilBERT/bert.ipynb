{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>plot</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"#7DaysLater\" (2013)</td>\n",
       "      <td>days later interactive comedy series featuring...</td>\n",
       "      <td>['Comedy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"#Cake\" (2015)</td>\n",
       "      <td>cake hour long serial narrative comedy manhunt...</td>\n",
       "      <td>['Comedy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"#DaddyLeaks\" (????)</td>\n",
       "      <td>life four close friends late thirties change f...</td>\n",
       "      <td>['Comedy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"#Elmira\" (2014)</td>\n",
       "      <td>elmira follows story bunch strangers respond c...</td>\n",
       "      <td>['Comedy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"#Fuga\" (2016)</td>\n",
       "      <td>months apocalyptic event group survivors find ...</td>\n",
       "      <td>['Action', 'Drama', 'Horror']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  movie                                               plot  \\\n",
       "0  \"#7DaysLater\" (2013)  days later interactive comedy series featuring...   \n",
       "1        \"#Cake\" (2015)  cake hour long serial narrative comedy manhunt...   \n",
       "2  \"#DaddyLeaks\" (????)  life four close friends late thirties change f...   \n",
       "3      \"#Elmira\" (2014)  elmira follows story bunch strangers respond c...   \n",
       "4        \"#Fuga\" (2016)  months apocalyptic event group survivors find ...   \n",
       "\n",
       "                          genres  \n",
       "0                     ['Comedy']  \n",
       "1                     ['Comedy']  \n",
       "2                     ['Comedy']  \n",
       "3                     ['Comedy']  \n",
       "4  ['Action', 'Drama', 'Horror']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/preprocessed_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get maximum length of the sentences in data['plot']\n",
    "max_len = np.max(data['plot'].apply(lambda x: len(x.split())))\n",
    "max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into torch datasets\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, plot, genre, tokenizer, max_len):\n",
    "        self.plot = plot\n",
    "        self.genre = genre\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # tokenize whole data at once and store in self.encodings\n",
    "        self.encodings = self.tokenizer.batch_encode_plus(\n",
    "            # convert self.plot to list\n",
    "            self.plot.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.plot)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        plot = str(self.plot[item])\n",
    "        genre = self.genre[item]\n",
    "\n",
    "        # encoding = self.tokenizer.encode_plus(\n",
    "        #     plot,\n",
    "        #     add_special_tokens=True,\n",
    "        #     max_length=self.max_len,\n",
    "        #     return_token_type_ids=False,\n",
    "        #     pad_to_max_length=True,\n",
    "        #     return_attention_mask=True,\n",
    "        #     return_tensors='pt'\n",
    "        # )\n",
    "\n",
    "        return {\n",
    "            'plot': plot,\n",
    "            'input_ids': self.encodings['input_ids'].flatten(),\n",
    "            'attention_mask': self.encodings['attention_mask'].flatten(),\n",
    "            'genre': torch.tensor(genre, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into MovieDataset\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenize function to data['plot'] in batches\n",
    "batch_size = 16\n",
    "train_size = 0.8\n",
    "\n",
    "train_data, test_data = train_test_split(data, train_size=train_size, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, train_size=train_size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "batch_text_or_text_pairs has to be a list or a tuple (got <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data_loader \u001b[39m=\u001b[39m MovieDataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     plot\u001b[39m=\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mplot\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto_numpy(),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     genre\u001b[39m=\u001b[39;49mdata[\u001b[39m'\u001b[39;49m\u001b[39mgenres\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto_numpy(),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     max_len\u001b[39m=\u001b[39;49mmax_len\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "\u001b[1;32m/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_len \u001b[39m=\u001b[39m max_len\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# tokenize whole data at once and store in self.encodings\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplot,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_len,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     pad_to_max_length\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home2/adyansh/project-aegifi/src/distilBERT/bert.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "File \u001b[0;32m~/project-aegifi/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3075\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3067\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3068\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3072\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3073\u001b[0m )\n\u001b[0;32m-> 3075\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3078\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3079\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3080\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3081\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3082\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3083\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3084\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3085\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3086\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3087\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3088\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3089\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3090\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3091\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
      "File \u001b[0;32m~/project-aegifi/env/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:491\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_encode_plus\u001b[39m(\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    471\u001b[0m     batch_text_or_text_pairs: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    489\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    490\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_text_or_text_pairs, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m--> 491\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_text_or_text_pairs has to be a list or a tuple (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(batch_text_or_text_pairs)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    497\u001b[0m         padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    498\u001b[0m         truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    502\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: batch_text_or_text_pairs has to be a list or a tuple (got <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "data_loader = MovieDataset(\n",
    "    plot=data['plot'].to_numpy(),\n",
    "    genre=data['genres'].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
