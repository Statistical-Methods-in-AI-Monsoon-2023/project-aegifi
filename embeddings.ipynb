{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Language Model\n",
    "\n",
    "Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability  of a given sequence of words occurring in a sentence. Thus, it may be defined as a probability distribution over the vocabulary words.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Importing the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 512  ,\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 3000,\n",
    "    'lr': 2*10e-6,\n",
    "    'num_epochs': 15,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the wandb project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nnlm__LR_8e-05_E_8_BS_1024_HD_5000_ED_300) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f768a021fba54e22963213de066a5e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.168414…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▅▃█▆▂▁▇▃</td></tr><tr><td>Training Loss</td><td>█▇▄▄▄▄▃▁</td></tr><tr><td>Training Perplexity</td><td>█▆▃▃▃▃▂▁</td></tr><tr><td>Validation Accuracy</td><td>▁▃▅▅▅█▇█</td></tr><tr><td>Validation Loss</td><td>█▄▂▁▁▁▁▂</td></tr><tr><td>Validation Perplexity</td><td>█▃▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>16.49485</td></tr><tr><td>Training Loss</td><td>4.05606</td></tr><tr><td>Training Perplexity</td><td>57.74614</td></tr><tr><td>Validation Accuracy</td><td>19.40299</td></tr><tr><td>Validation Loss</td><td>5.24244</td></tr><tr><td>Validation Perplexity</td><td>189.13149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nnlm__LR_8e-05_E_8_BS_1024_HD_5000_ED_300</strong> at: <a href='https://wandb.ai/wb-team-hardik/nnlm/runs/nnlm__LR_8e-05_E_8_BS_1024_HD_5000_ED_300' target=\"_blank\">https://wandb.ai/wb-team-hardik/nnlm/runs/nnlm__LR_8e-05_E_8_BS_1024_HD_5000_ED_300</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230903_184154-nnlm__LR_8e-05_E_8_BS_1024_HD_5000_ED_300/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nnlm__LR_8e-05_E_8_BS_1024_HD_5000_ED_300). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e65d76481d44db8928b3474de6c5985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668978399987586, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hardk/AMNESIA/College/Sem 5/ANLP/Assignments/Assignment 1/Neural Network Language Model/wandb/run-20230903_191709-nnlm__LR_2e-05_E_16_BS_512_HD_3000_ED_300</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wb-team-hardik/nnlm/runs/nnlm__LR_2e-05_E_16_BS_512_HD_3000_ED_300' target=\"_blank\">nnlm__LR_2e-05_E_16_BS_512_HD_3000_ED_300</a></strong> to <a href='https://wandb.ai/wb-team-hardik/nnlm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wb-team-hardik/nnlm' target=\"_blank\">https://wandb.ai/wb-team-hardik/nnlm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wb-team-hardik/nnlm/runs/nnlm__LR_2e-05_E_16_BS_512_HD_3000_ED_300' target=\"_blank\">https://wandb.ai/wb-team-hardik/nnlm/runs/nnlm__LR_2e-05_E_16_BS_512_HD_3000_ED_300</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "run = wandb.init(project='nnlm', config=config,\n",
    "                 id=f'nnlm__LR_{config[\"lr\"]}_E_{config[\"num_epochs\"]}_BS_{config[\"batch_size\"]}_HD_{config[\"hidden_dim\"]}_ED_{config[\"embedding_dim\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to preprocess and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z;.]+\", ' ', text)\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.replace(';', ' ; ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    tokens = word_tokenize(text)\n",
    "    return text, tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I.\n",
      "The Shade of Cardinal Richelieu.\n",
      "\n",
      "\n",
      "In a splendid chamber of the Palais Royal, formerly st\n"
     ]
    }
   ],
   "source": [
    "corpus_data = open('Dataset/Auguste_Maquet.txt', 'r').read()\n",
    "print(corpus_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  1014605\n",
      "Number of . and ; in the corpus:  58690\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus, tokens = preprocess(corpus_data)\n",
    "\n",
    "# print(cleaned_corpus[:500], '\\n') \n",
    "# print(tokens[:50])\n",
    "print(\"Number of tokens: \", len(tokens))\n",
    "print(\"Number of . and ; in the corpus: \", tokens.count('.')+tokens.count(';'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the corpus:  58690\n",
      "Number of training sentences:  41083\n",
      "Number of validation sentences:  5868\n",
      "Number of testing sentences:  11739\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the sentences from the corpus and split them into train, validation and test sets\n",
    "\"\"\"\n",
    "\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for token in tokens:\n",
    "    if token == '.' or token == ';':\n",
    "        # if(len(current_sentence) > 5):\n",
    "        current_sentence.append(\"<EOS>\")\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = [\"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\", \"<PAD>\"]\n",
    "    else:\n",
    "        current_sentence.append(token)\n",
    "\n",
    "print('Number of sentences in the corpus: ', len(sentences))\n",
    "\n",
    "with open('Dataset/sentences.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "train_sentences, test_valid_sentences = train_test_split(\n",
    "    sentences, test_size=0.3, random_state=42)\n",
    "valid_sentences, test_sentences = train_test_split(\n",
    "    test_valid_sentences, test_size=0.6667, random_state=42)\n",
    "\n",
    "print('Number of training sentences: ', len(train_sentences))\n",
    "print('Number of validation sentences: ', len(valid_sentences))\n",
    "print('Number of testing sentences: ', len(test_sentences))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  18472\n",
      "Most common words:  [('<PAD>', 205410), ('<EOS>', 41083), ('the', 39613), ('to', 18798), ('of', 17867), ('and', 17443), ('a', 13312), ('i', 11447), ('you', 11292), ('he', 9693)]\n",
      "Least common words:  [('hops', 1), ('gin', 1), ('rotund', 1), ('canonical', 1), ('lamorici', 1), ('ney', 1), ('changarnier', 1), ('bedeau', 1), ('supplicatingly', 1), ('jaw', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_counter = Counter([word for sentence in train_sentences for word in sentence])\n",
    "\n",
    "# remove the words that appear only once\n",
    "# vocab_counter = Counter({word: freq for word, freq in vocab_counter.items() if freq > 1})\n",
    "\n",
    "vocab = [\"<PAD>\", \"<UNK>\", \"<EOS>\"] + [word for word, freq in vocab_counter.items()]\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size: \", len(vocab))\n",
    "print(\"Most common words: \", vocab_counter.most_common(10))\n",
    "print(\"Least common words: \", vocab_counter.most_common()[-10:])\n",
    "\n",
    "\n",
    "# save the vocab as npy file\n",
    "np.save('Dataset/vocab.npy', vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(api.info()['models'].keys()))\n",
    "if not 'word_vectors' in locals():  # so that it doesn't load again if it was already loaded\n",
    "    word_vectors = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown words:  1126\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the embedding matrix for the vocab\n",
    "\"\"\"\n",
    "\n",
    "embedding_matrix = []\n",
    "num_unknown_words = 0\n",
    "for word in vocab:\n",
    "    if word in word_vectors:\n",
    "        # embedding_matrix.append(word_vectors[word])\n",
    "        # print(\"Word {} found in w2v dict\".format(word))\n",
    "        if len(word_vectors[word]) != config[\"embedding_dim\"]:\n",
    "            print(\"Word {} size {} does not match embedding dim {}\".format(\n",
    "                word, len(word_vectors[word]), config[\"embedding_dim\"]))\n",
    "        else:\n",
    "            embedding_matrix.append(word_vectors[word])\n",
    "    else:\n",
    "        # print(\"Word {} not in w2v dict\".format(word))\n",
    "        num_unknown_words += 1\n",
    "        embedding_matrix.append([0]*config[\"embedding_dim\"])\n",
    "\n",
    "print(\"Number of unknown words: \", num_unknown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix size:  18472\n",
      "Embedding size:  300\n",
      "Embedding matrix first row:  [ 2.6926e-01  3.8534e-01 -6.5080e-01  7.9847e-02  1.4793e-01 -7.8472e-02\n",
      "  5.3576e-02 -6.6062e-02  4.2256e-01 -1.3911e+00  3.5039e-01 -3.0858e-01\n",
      "  4.1294e-01  5.1396e-02 -1.8945e-01 -1.3053e-01  2.2396e-01  3.2059e-01\n",
      " -1.4995e-01  3.9264e-02  5.9915e-02 -2.0883e-01  5.2932e-01 -4.0698e-01\n",
      " -2.8060e-01 -2.5013e-01  3.9860e-01 -1.5108e-01  3.2029e-01  8.5095e-02\n",
      " -1.2038e-01 -5.5589e-01  3.0680e-01 -1.4087e-01 -1.2590e+00  1.1816e-01\n",
      "  1.4410e-01  5.7897e-01 -4.7504e-01 -3.6376e-01  1.9966e-01  3.7050e-01\n",
      " -1.9161e-02  1.8794e-01  5.0041e-02  2.5054e-01  9.8075e-03 -7.6851e-02\n",
      " -2.2045e-02 -3.8040e-01  2.4530e-01 -7.3199e-02  8.3152e-02 -4.3383e-01\n",
      " -2.0633e-01  4.8931e-01 -5.0676e-01  3.8061e-01 -2.2221e-01  3.8953e-02\n",
      "  4.3099e-01 -4.9454e-04 -3.6134e-01  1.3887e-01  8.0764e-02 -2.7260e-01\n",
      " -3.2459e-02  1.7780e-01  4.4219e-02  3.7202e-01 -1.0937e-01 -1.2273e-01\n",
      "  5.8154e-02  3.3571e-01  2.5524e-01 -4.0259e-01 -5.8757e-02 -3.7601e-02\n",
      "  4.4650e-01 -6.1849e-01  1.8575e-01 -3.3270e-01  4.8101e-01 -2.7793e-02\n",
      " -3.7576e-01  7.6359e-02 -2.5737e-01  5.1384e-01 -5.1345e-02 -2.1220e-01\n",
      " -4.6083e-01  6.2232e-01 -9.2879e-02 -4.8831e-01  1.9148e-04 -2.9304e-01\n",
      " -2.5424e-01 -5.1010e-02  2.8212e-02 -2.9697e-01 -3.9366e-01 -1.1627e-01\n",
      " -4.2398e-02  2.9439e-01  1.6329e-01 -1.8744e-01 -1.7032e-01 -3.0815e-01\n",
      "  5.2788e-01  1.5281e-01  4.1023e-01 -1.9633e-01 -4.8761e-01 -5.7598e-01\n",
      "  1.3854e-01  3.0509e-01 -1.5280e-01  3.0348e-01  2.5335e-01 -1.7639e-01\n",
      "  9.7629e-02 -2.8491e-02  3.6629e-01 -3.7006e-01 -2.6543e-01 -3.2905e-01\n",
      "  5.8286e-01  5.2319e-01  2.3366e-01 -4.5052e-02  2.2859e-01  8.4578e-02\n",
      "  8.4230e-02  4.2591e-01  4.1195e-01  1.6220e-01 -2.2863e-01 -4.1000e-01\n",
      " -2.8883e-01 -9.9117e-02  5.5991e-02 -2.7556e-02 -1.5026e-01  3.9955e-01\n",
      " -9.9367e-01  9.2932e-02 -1.4091e-01 -2.7166e-01 -4.8311e-02  1.7841e-01\n",
      " -4.6431e-01  2.6857e-02  2.6575e-01 -1.9988e-01  2.6734e-01  1.0021e-01\n",
      " -3.7684e-01 -1.2756e-01  2.3577e-01  3.5088e-01  6.0882e-01 -5.4958e-01\n",
      "  1.3567e-01  1.5461e-01  6.1223e-02 -8.0811e-02  5.5888e-02  2.7606e-02\n",
      " -1.3213e-01  1.4792e-01 -1.7589e-01 -1.1839e-01 -3.2821e-01 -2.5180e-01\n",
      "  4.7843e-02  2.0705e-01 -3.2478e-01  1.4081e-01 -8.1057e-02  3.2312e-01\n",
      " -2.1633e-01 -1.6537e-01  2.7887e-01  2.7317e-01 -1.3813e-01  3.9700e-01\n",
      " -7.6695e-02 -3.7658e-02  3.4230e-01 -3.9727e-02  1.2255e-01  2.6669e-01\n",
      " -1.2029e-02  6.7727e-02  4.0615e-01 -2.8511e-01  1.0652e-02  2.5626e-01\n",
      "  1.3464e-01 -2.5784e-01  9.6414e-01  5.7625e-01  2.2879e-01 -1.1149e-01\n",
      "  3.9451e-02  2.7942e-01  1.0049e-02  1.7583e-01 -4.8021e-01 -3.6665e-02\n",
      "  2.2427e-01 -1.1210e-01  4.5474e-01  4.8511e-01  4.6550e-02 -7.2412e-02\n",
      "  2.3225e-01  1.5329e-01 -5.3602e-02 -6.9617e-03  6.6677e-01 -2.7539e-02\n",
      " -2.1151e-01 -1.2863e-02 -1.2992e-01 -6.5766e-01  1.5430e-01  1.9427e-01\n",
      " -4.6203e-02  3.4428e-02  7.3659e-02  3.8398e-01  3.2040e-01  3.1329e-03\n",
      "  1.6577e-01  1.3169e-01 -3.2086e-01  4.5876e-01  1.5348e-01 -2.2698e-01\n",
      " -1.4886e-01 -1.3274e-02  1.5452e-01  1.1949e-01 -4.0030e-01  1.2097e-02\n",
      "  6.6531e-01  4.7685e-01 -3.3378e-01 -4.7221e-02  3.4085e-01 -3.0376e-01\n",
      "  1.8955e-01  9.0133e-02  5.0423e-01  2.1918e-01 -4.8407e-02  1.1840e-01\n",
      " -1.4385e-01  2.6000e-01  1.7508e-01 -3.6159e-01  7.4430e-03  2.3947e-01\n",
      " -3.1497e-03 -9.1092e-03 -3.8100e-01 -3.2759e-01  2.1251e-01 -1.2253e-01\n",
      "  2.0958e-01  3.7491e-01  3.2324e-01  3.0070e-01 -1.9796e-01 -2.2254e-01\n",
      " -2.1465e+00  4.4136e-01  4.7770e-01 -1.7422e-01  1.2803e-01  7.2328e-03\n",
      "  6.4197e-02  3.8880e-01  1.9865e-01 -5.7883e-01  1.6371e-01  2.2076e-01\n",
      " -1.2237e-01  1.4758e-01  1.3936e-01 -7.2021e-01  4.0467e-01 -7.3453e-02\n",
      "  5.0288e-01 -7.8705e-01 -2.9255e-02 -5.6927e-03  2.4317e-01  2.2824e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding matrix size: \", len(embedding_matrix))\n",
    "print(\"Embedding size: \", len(embedding_matrix[0]))\n",
    "print(\"Embedding matrix first row: \", embedding_matrix[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape:  torch.Size([18472, 300])\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.array(embedding_matrix) # since converting a list directly to tensor is very slow\n",
    "embedding_matrix = torch.tensor(embedding_matrix)\n",
    "print(\"Embedding matrix shape: \", embedding_matrix.shape)\n",
    "\n",
    "# save the embedding matrix locally as npy file\n",
    "np.save('Dataset/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data and the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  708705\n",
      "Sample training data:  ([45, 46, 6, 47, 48], 49)\n",
      "Number of batches in the training set:  1385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NNLM_Dataset(Dataset):\n",
    "    def __init__(self, sentences, word2idx):\n",
    "        self.data = []\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)-5):\n",
    "                context = sentence[i:i+5]\n",
    "                target = sentence[i+5]\n",
    "                context_idxs = [word2idx[word] if word in word2idx  else word2idx[\"<UNK>\"]  for word in context]\n",
    "                target_idx = word2idx[target] if target in word2idx else word2idx[\"<UNK>\"]\n",
    "                self.data.append((context_idxs, target_idx))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = NNLM_Dataset(train_sentences, word2idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "print(\"Number of training samples: \", len(train_dataset))\n",
    "print(\"Sample training data: \", train_dataset[57])\n",
    "print(\"Number of batches in the training set: \", len(train_loader))\n",
    "\n",
    "valid_dataset = NNLM_Dataset(valid_sentences, word2idx)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "test_dataset = NNLM_Dataset(test_sentences, word2idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(nn.Module):\n",
    "    def __init__(self, embedding_matrix, vocab_size, embedding_dim = config[\"embedding_dim\"], hidden_dim = config[\"hidden_dim\"]):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.fc1 = nn.Linear(5*embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # self.fc3 = nn.Linear(hidden_dim,1000)\n",
    "        # self.fc4 = nn.Linear(1000,7500)\n",
    "        # self.fc2 = nn.Linear(7500, vocab_size)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.bn1 = nn.BatchNorm1d(5*embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)        \n",
    "        x= x.view(x.size(0), -1)\n",
    "\n",
    "        # adding batch normalization\n",
    "        # x = self.bn1(x)\n",
    "\n",
    "        x=x.float()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = NNLM(embedding_matrix, len(vocab), config['embedding_dim'], config['hidden_dim'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:42<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16 - Training loss: 6.172402858734131, Training accuracy: 10.309278350515463, Training perplexity: 479.3365013159206\n",
      "Epoch 1/16 - Validation loss: 6.304048556907504, Validation accuracy: 12.5, Validation perplexity: 546.7811095114712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:42<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/16 - Training loss: 6.160462856292725, Training accuracy: 13.402061855670103, Training perplexity: 473.6472547189058\n",
      "Epoch 2/16 - Validation loss: 5.991466802709243, Validation accuracy: 25.0, Validation perplexity: 400.00090224152194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:41<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/16 - Training loss: 5.527929782867432, Training accuracy: 15.463917525773196, Training perplexity: 251.62245832543053\n",
      "Epoch 3/16 - Validation loss: 5.805563396098567, Validation accuracy: 25.0, Validation perplexity: 332.14226829807683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:36<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/16 - Training loss: 4.8345794677734375, Training accuracy: 17.525773195876287, Training perplexity: 125.78567515284\n",
      "Epoch 4/16 - Validation loss: 5.668761669420729, Validation accuracy: 25.0, Validation perplexity: 289.67559804554736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:20<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/16 - Training loss: 5.08284854888916, Training accuracy: 21.649484536082475, Training perplexity: 161.2326815857154\n",
      "Epoch 5/16 - Validation loss: 5.563753733447954, Validation accuracy: 25.0, Validation perplexity: 260.7999748055362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:35<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/16 - Training loss: 5.500262260437012, Training accuracy: 10.309278350515463, Training perplexity: 244.75611369306617\n",
      "Epoch 6/16 - Validation loss: 5.489057424021702, Validation accuracy: 20.833333333333336, Validation perplexity: 242.02896861704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:42<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/16 - Training loss: 4.887378692626953, Training accuracy: 17.525773195876287, Training perplexity: 132.6055182472701\n",
      "Epoch 7/16 - Validation loss: 5.438819950702143, Validation accuracy: 16.666666666666664, Validation perplexity: 230.1704107080131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:42<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/16 - Training loss: 4.944179058074951, Training accuracy: 19.587628865979383, Training perplexity: 140.35557976264357\n",
      "Epoch 8/16 - Validation loss: 5.404058535893758, Validation accuracy: 16.666666666666664, Validation perplexity: 222.30682803237855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:41<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/16 - Training loss: 4.635165691375732, Training accuracy: 14.432989690721648, Training perplexity: 103.04499024702416\n",
      "Epoch 9/16 - Validation loss: 5.379032763780332, Validation accuracy: 16.666666666666664, Validation perplexity: 216.81246510410972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:41<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/16 - Training loss: 4.796623229980469, Training accuracy: 20.618556701030926, Training perplexity: 121.10079677055536\n",
      "Epoch 10/16 - Validation loss: 5.363888801312914, Validation accuracy: 16.666666666666664, Validation perplexity: 213.5538020834785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:40<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/16 - Training loss: 4.867167949676514, Training accuracy: 19.587628865979383, Training perplexity: 129.9523636286379\n",
      "Epoch 11/16 - Validation loss: 5.352298816045125, Validation accuracy: 16.666666666666664, Validation perplexity: 211.09300451330014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:39<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/16 - Training loss: 4.950023651123047, Training accuracy: 14.432989690721648, Training perplexity: 141.1783029074049\n",
      "Epoch 12/16 - Validation loss: 5.3453649633071, Validation accuracy: 16.666666666666664, Validation perplexity: 209.63437949552886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:43<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/16 - Training loss: 4.943075180053711, Training accuracy: 15.463917525773196, Training perplexity: 140.2007298064411\n",
      "Epoch 13/16 - Validation loss: 5.34230072591819, Validation accuracy: 16.666666666666664, Validation perplexity: 208.9929931737125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:42<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/16 - Training loss: 4.698666095733643, Training accuracy: 12.371134020618557, Training perplexity: 109.80061122077294\n",
      "Epoch 14/16 - Validation loss: 5.344267188333998, Validation accuracy: 12.5, Validation perplexity: 209.4043743902394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:38<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/16 - Training loss: 4.4259138107299805, Training accuracy: 17.525773195876287, Training perplexity: 83.589157000682\n",
      "Epoch 15/16 - Validation loss: 5.347511950661154, Validation accuracy: 12.5, Validation perplexity: 210.0849453638158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1385/1385 [02:32<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/16 - Training loss: 4.7526984214782715, Training accuracy: 12.371134020618557, Training perplexity: 115.89660083411142\n",
      "Epoch 16/16 - Validation loss: 5.354659933669894, Validation accuracy: 12.5, Validation perplexity: 211.59200879880473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(\"Device: \", device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    for context_idxs, target_idx in tqdm(train_loader):\n",
    "    \n",
    "        context_idxs, target_idx = np.squeeze(context_idxs), np.squeeze(target_idx) # remove the extra dimension\n",
    "        context_idxs, target_idx = np.array(context_idxs), np.array(target_idx) # convert to numpy array\n",
    "        context_idxs, target_idx = torch.from_numpy(context_idxs), torch.from_numpy(target_idx) # convert to torch tensor\n",
    "        context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)   # move to device\n",
    "        context_idxs = context_idxs.transpose(1,0)  # transpose to get the batch size first\n",
    "\n",
    "        optimizer.zero_grad()   # zero the gradients\n",
    "        log_probs = model(context_idxs) # forward pass\n",
    "        loss = criterion(log_probs, target_idx) # calculate the loss\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step()    # update the weights\n",
    "\n",
    "    training_loss = loss.item()\n",
    "    training_acc = (torch.argmax(log_probs, dim=1) == target_idx).sum().item()/len(target_idx)* 100\n",
    "    training_perplexity = np.exp(training_loss)\n",
    "    print (f\"Epoch {epoch+1}/{config['num_epochs']} - Training loss: {training_loss}, Training accuracy: {training_acc}, Training perplexity: {training_perplexity}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context_idxs, target_idx in valid_loader:\n",
    "            context_idxs, target_idx = np.squeeze(context_idxs), np.squeeze(target_idx)\n",
    "            context_idxs, target_idx = np.array(context_idxs), np.array(target_idx)\n",
    "            context_idxs, target_idx = torch.from_numpy(context_idxs), torch.from_numpy(target_idx)\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            context_idxs = context_idxs.transpose(1,0)\n",
    "            \n",
    "            log_probs = model(context_idxs)\n",
    "            loss = criterion(log_probs, target_idx)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    \n",
    "    validation_loss = valid_loss/len(valid_loader)\n",
    "    validation_acc = (torch.argmax(log_probs, dim=1) == target_idx).sum().item()/len(target_idx)* 100\n",
    "    validation_perplexity = np.exp(validation_loss)\n",
    "    print (f\"Epoch {epoch+1}/{config['num_epochs']} - Validation loss: {validation_loss}, Validation accuracy: {validation_acc}, Validation perplexity: {validation_perplexity}\")\n",
    "    \n",
    "    wandb.log({\"Training Loss\": training_loss, \"Training Accuracy\": training_acc, \"Training Perplexity\": training_perplexity,\n",
    "                \"Validation Loss\": validation_loss, \"Validation Accuracy\": validation_acc, \"Validation Perplexity\": validation_perplexity})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate the perplexity of the model\n",
    "\"\"\"\n",
    "\n",
    "def calculate_perplexity(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for context_idxs, target_idx in data_loader:\n",
    "            context_idxs, target_idx = np.squeeze(context_idxs), np.squeeze(target_idx)\n",
    "            context_idxs, target_idx = np.array(context_idxs), np.array(target_idx)\n",
    "            context_idxs, target_idx = torch.from_numpy(context_idxs), torch.from_numpy(target_idx)\n",
    "\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            context_idxs = context_idxs.transpose(1,0)\n",
    "            log_probs = model(context_idxs)\n",
    "            loss = criterion(log_probs, target_idx)\n",
    "            total_loss += loss.item()\n",
    "    return np.exp(total_loss/len(data_loader)), total_loss/len(data_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the perplexity on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on the train set:  87.71125950604727\n",
      "Loss on the train set:  4.474050277751275\n"
     ]
    }
   ],
   "source": [
    "# Calculate the perplexity on the train set\n",
    "\n",
    "perplexity_train, loss_train = calculate_perplexity(model, train_loader, criterion)\n",
    "print(\"Perplexity on the train set: \", perplexity_train)\n",
    "print(\"Loss on the train set: \", loss_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the perplexity on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on the validation set:  211.59200879880473\n",
      "Loss on the validation set:  5.354659933669894\n"
     ]
    }
   ],
   "source": [
    "# Calculate the perplexity on the validation set\n",
    "perplexity_valid, loss_valid = calculate_perplexity(model, valid_loader, criterion)\n",
    "print(\"Perplexity on the validation set: \", perplexity_valid)\n",
    "print(\"Loss on the validation set: \", loss_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the perplexity on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on the test set:  211.75566290861588\n",
      "Loss on the test set:  5.355433076544653\n"
     ]
    }
   ],
   "source": [
    "# Calculate the perplexity on the test set\n",
    "perplexity_test, loss_test = calculate_perplexity(model, test_loader, criterion)\n",
    "print(\"Perplexity on the test set: \", perplexity_test)\n",
    "print(\"Loss on the test set: \", loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating and saving sentence wise perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report sentence wise perplexity on the test set\n",
    "def calculate_sentence_perplexity(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    sentence_perplexity = []\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for context_idxs, target_idx in data_loader:\n",
    "            context_idxs, target_idx = np.squeeze(context_idxs), np.squeeze(target_idx)\n",
    "            context_idxs, target_idx = np.array(context_idxs), np.array(target_idx)\n",
    "            context_idxs, target_idx = torch.from_numpy(context_idxs), torch.from_numpy(target_idx)\n",
    "\n",
    "            context_idxs, target_idx = context_idxs.to(device), target_idx.to(device)\n",
    "            context_idxs = context_idxs.transpose(1,0)\n",
    "            log_probs = model(context_idxs)\n",
    "            loss = criterion(log_probs, target_idx)\n",
    "            cnt += 1\n",
    "            \n",
    "            temp_str = str(cnt) + \"\\t:\\t\" + str(np.exp(loss.item()))\n",
    "            sentence_perplexity.append(temp_str)\n",
    "    return sentence_perplexity\n",
    "\n",
    "sentence_perplexity_test = calculate_sentence_perplexity(model, test_loader, criterion)\n",
    "sentence_perplexity_train = calculate_sentence_perplexity(model, train_loader, criterion)\n",
    "\n",
    "# store the sentence wise perplexity in a file\n",
    "with open('Results/2021114016_LM1_test_perplexity.txt', 'w') as f:\n",
    "    for perplexity in sentence_perplexity_test:\n",
    "        f.write(str(perplexity) + '\\n')\n",
    "\n",
    "with open('Results/2021114016_LM1_train_perplexity.txt', 'w') as f:\n",
    "    for perplexity in sentence_perplexity_train:\n",
    "        f.write(str(perplexity) + '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the parameters along with the results in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saving the parameters along with the results in a file\n",
    "\"\"\"\n",
    "with open('results.txt', 'a') as f:\n",
    "    f.write(f\"Learning rate : {config['lr']}\\n\")\n",
    "    f.write(f\"Number of epochs : {config['num_epochs']}\\n\")\n",
    "    f.write(f\"Batch size : {config['batch_size']}\\n\")\n",
    "    f.write(f\"Hidden dimension : {config['hidden_dim']}\\n\")\n",
    "    f.write(f\"Embedding dimension : {config['embedding_dim']}\\n\")\n",
    "    f.write(f\"Perplexity on the train set: {perplexity_train}\\n\")\n",
    "    f.write(f\"Perplexity on the validation set: {perplexity_valid}\\n\")\n",
    "    f.write(f\"Perplexity on the test set: {perplexity_test}\\n\")\n",
    "    f.write(f\"Loss on the train set: {loss_train}\\n\")\n",
    "    f.write(f\"Loss on the validation set: {loss_valid}\\n\")\n",
    "    f.write(f\"Loss on the test set: {loss_test}\\n\")\n",
    "    f.write(\"--------------------------------------------------\\n\")\n",
    "    f.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'nnlm.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "316 : 5*10^-6\n",
    "<br>\n",
    "286 : 10^-5\n",
    "<br>\n",
    "275 : 8*10^-6\n",
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
